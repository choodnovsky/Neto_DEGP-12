{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e589382b",
   "metadata": {},
   "source": [
    "### Домашнее задание №5 к лекции \"Основы веб-скрапинга\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ed03b14e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from time import sleep\n",
    "from datetime import datetime\n",
    "import re\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f744adcc",
   "metadata": {},
   "source": [
    "#### Задание 1\n",
    "Вам необходимо написать функцию, которая будет основана на поиске по сайту http://habr.com. Функция в качестве параметра должна принимать список запросов для поиска (например, ['python', 'анализ данных']) и на основе материалов, попавших в результаты поиска по каждому запросу, возвращать датафрейм вида:\n",
    "\n",
    "<дата> - <заголовок> - <ссылка на материал>\n",
    "В рамках задания предполагается работа только с одной (первой) страницей результатов поисковой выдачи для каждого запроса. Материалы в датафрейме не должны дублироваться, если они попадали в результаты поиска для нескольких запросов из списка."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e755d365",
   "metadata": {},
   "outputs": [],
   "source": [
    "req = ['большие данные', 'python', 'sql', 'big data']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4b5ab9e",
   "metadata": {},
   "source": [
    "Так как HTML извлекать придется во каждом случае, создадим для этого вспомогательную функцию."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4dcc7c66",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_html(url, params=None):\n",
    "    ''' \n",
    "    Функция для получения HTML кода из url в случае 200 ответа сайта.\n",
    "    В противном - ничего\n",
    "    '''\n",
    "    html = requests.get(url, params=params)\n",
    "    if html.status_code == 200:\n",
    "        return html.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "576b43c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_content_lite(req):\n",
    "    '''\n",
    "    Функия получения короткой таблицы со статьями по списку поисковых слов\n",
    "    На вход принимает список слов.\n",
    "    Внутри:\n",
    "    1) для каждого слова делает запрос к сайту\n",
    "    2) перебирает блоки со статьями на первой странице поиска\n",
    "    3) для каждой статьи собирает дату, название, ссылку, автора\n",
    "    4) добавляет конкретное поисковое слово\n",
    "    5) доавляет полученную строку к датафрейму\n",
    "    6) проходит в цикле по всем словам из переданного списка\n",
    "    7) удаляет дубли по столбцу со ссылками\n",
    "    8) работает только для первой страницы поиска\n",
    "    '''\n",
    "    url = 'https://habr.com/ru/search/'\n",
    "    df = pd.DataFrame()\n",
    "    for word in req:\n",
    "        params = {'q': word}\n",
    "        html = get_html(url, params=params) # используем вспомогательную функцию для получения html\n",
    "        soup = BeautifulSoup(html, 'html.parser')\n",
    "        articles = soup.find('div', class_='tm-articles-list').find_all('div', 'tm-article-snippet')\n",
    "        for item in tqdm(articles):\n",
    "            sleep(0.1)\n",
    "            date = pd.to_datetime(item.find('time').get('datetime')).strftime('%Y-%m-%d %H:%M') \n",
    "            title = item.find('a', 'tm-article-snippet__title-link').text\n",
    "            link = 'https://habr.com' + item.find('a', 'tm-article-snippet__title-link').get('href')\n",
    "            author = item.find('span', 'tm-user-info__user').text.strip()\n",
    "            row = {'word':word, # собираем строку\n",
    "                   'date':date, \n",
    "                   'title':title, \n",
    "                   'link':link, \n",
    "                   'author':author}\n",
    "            df = df.append(row, ignore_index=True) # добавляем строку\n",
    "    df.drop_duplicates(subset='link', inplace=True) # удаляем дубли\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ce1b6679",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 19/19 [00:02<00:00,  8.80it/s]\n",
      "100%|██████████| 20/20 [00:02<00:00,  8.82it/s]\n",
      "100%|██████████| 20/20 [00:02<00:00,  8.74it/s]\n",
      "100%|██████████| 19/19 [00:02<00:00,  8.85it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.15 s, sys: 102 ms, total: 1.26 s\n",
      "Wall time: 12.3 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "content_lite = get_content_lite(req)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bcecd7c7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word</th>\n",
       "      <th>date</th>\n",
       "      <th>title</th>\n",
       "      <th>link</th>\n",
       "      <th>author</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>python</td>\n",
       "      <td>2022-05-18 15:33</td>\n",
       "      <td>Слёрм запускает 3-дневный интенсив по Python д...</td>\n",
       "      <td>https://habr.com/ru/company/southbridge/news/t...</td>\n",
       "      <td>edeshina</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>python</td>\n",
       "      <td>2022-04-22 11:42</td>\n",
       "      <td>TechnoMeetsPython. Онлайн митап о Python-разра...</td>\n",
       "      <td>https://habr.com/ru/news/t/662437/</td>\n",
       "      <td>technokratiya</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>большие данные</td>\n",
       "      <td>2022-04-07 14:23</td>\n",
       "      <td>17 лучших инструментов и технологий для работы...</td>\n",
       "      <td>https://habr.com/ru/company/otus/blog/659657/</td>\n",
       "      <td>kmoseenk</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>sql</td>\n",
       "      <td>2022-04-07 08:56</td>\n",
       "      <td>Яндекс Практикум запускает курс «SQL для работ...</td>\n",
       "      <td>https://habr.com/ru/company/yandex_praktikum/n...</td>\n",
       "      <td>eshulyndina</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>python</td>\n",
       "      <td>2022-03-18 15:31</td>\n",
       "      <td>24 марта Слёрм проведёт открытый урок «Первый ...</td>\n",
       "      <td>https://habr.com/ru/company/southbridge/news/t...</td>\n",
       "      <td>edeshina</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62</th>\n",
       "      <td>big data</td>\n",
       "      <td>2022-03-10 08:30</td>\n",
       "      <td>10—24 марта: Big Data Dev Week от билайна</td>\n",
       "      <td>https://habr.com/ru/company/beeline/news/t/654...</td>\n",
       "      <td>Bee_brightside</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>python</td>\n",
       "      <td>2022-03-08 09:13</td>\n",
       "      <td>Вышел мартовский релиз расширения Python для V...</td>\n",
       "      <td>https://habr.com/ru/news/t/654707/</td>\n",
       "      <td>maybe_elf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74</th>\n",
       "      <td>big data</td>\n",
       "      <td>2022-02-18 12:51</td>\n",
       "      <td>Citymobil Data Meetup №7</td>\n",
       "      <td>https://habr.com/ru/company/citymobil/news/t/6...</td>\n",
       "      <td>leleles</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>python</td>\n",
       "      <td>2022-01-20 15:37</td>\n",
       "      <td>Курс «Python для инженеров». Старт 3 потока 31...</td>\n",
       "      <td>https://habr.com/ru/company/southbridge/news/t...</td>\n",
       "      <td>Hedgehog_art</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63</th>\n",
       "      <td>big data</td>\n",
       "      <td>2022-01-20 11:02</td>\n",
       "      <td>Citymobil Data Meetup №6</td>\n",
       "      <td>https://habr.com/ru/company/citymobil/news/t/6...</td>\n",
       "      <td>leleles</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              word              date  \\\n",
       "35          python  2022-05-18 15:33   \n",
       "37          python  2022-04-22 11:42   \n",
       "13  большие данные  2022-04-07 14:23   \n",
       "40             sql  2022-04-07 08:56   \n",
       "36          python  2022-03-18 15:31   \n",
       "62        big data  2022-03-10 08:30   \n",
       "25          python  2022-03-08 09:13   \n",
       "74        big data  2022-02-18 12:51   \n",
       "19          python  2022-01-20 15:37   \n",
       "63        big data  2022-01-20 11:02   \n",
       "\n",
       "                                                title  \\\n",
       "35  Слёрм запускает 3-дневный интенсив по Python д...   \n",
       "37  TechnoMeetsPython. Онлайн митап о Python-разра...   \n",
       "13  17 лучших инструментов и технологий для работы...   \n",
       "40  Яндекс Практикум запускает курс «SQL для работ...   \n",
       "36  24 марта Слёрм проведёт открытый урок «Первый ...   \n",
       "62          10—24 марта: Big Data Dev Week от билайна   \n",
       "25  Вышел мартовский релиз расширения Python для V...   \n",
       "74                           Citymobil Data Meetup №7   \n",
       "19  Курс «Python для инженеров». Старт 3 потока 31...   \n",
       "63                           Citymobil Data Meetup №6   \n",
       "\n",
       "                                                 link          author  \n",
       "35  https://habr.com/ru/company/southbridge/news/t...        edeshina  \n",
       "37                 https://habr.com/ru/news/t/662437/   technokratiya  \n",
       "13      https://habr.com/ru/company/otus/blog/659657/        kmoseenk  \n",
       "40  https://habr.com/ru/company/yandex_praktikum/n...     eshulyndina  \n",
       "36  https://habr.com/ru/company/southbridge/news/t...        edeshina  \n",
       "62  https://habr.com/ru/company/beeline/news/t/654...  Bee_brightside  \n",
       "25                 https://habr.com/ru/news/t/654707/       maybe_elf  \n",
       "74  https://habr.com/ru/company/citymobil/news/t/6...         leleles  \n",
       "19  https://habr.com/ru/company/southbridge/news/t...    Hedgehog_art  \n",
       "63  https://habr.com/ru/company/citymobil/news/t/6...         leleles  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "content_lite.sort_values(by='date', ascending=False).head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd7f3313",
   "metadata": {},
   "source": [
    "Для каждого поискового слова скрипт срабатывает за 2 секунды"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cfb1a27",
   "metadata": {},
   "source": [
    "#### Задание 2  \n",
    "Функция из обязательной части задания должна быть расширена следующим образом:\n",
    "\n",
    "кроме списка ключевых слов для поиска необходимо объявить параметр с количеством страниц поисковой выдачи. Т.е. при передаче в функцию аргумента 4 необходимо получить материалы с первых 4 страниц результатов;\n",
    "в датафрейме должны быть столбцы с полным текстом найденных материалов и количеством лайков:\n",
    "<дата> - <заголовок> - <ссылка на материал> - <текст материала> - <количество лайков>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4f5cff18",
   "metadata": {},
   "outputs": [],
   "source": [
    "req = ['python', 'mlflow', 'airflow', 'sql']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "673a25ed",
   "metadata": {},
   "source": [
    "Добавим еще две вспомогательные функции.  \n",
    "\n",
    "1) Для определения количества страниц поиска (это максимальное количество, дальше поиск работать не будет)  \n",
    "2) Для получения полного текста статьи и количества оценок"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5898831b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_num_pagins(url, word):\n",
    "    '''Фукнция принимает на вход url и поисковое слово.\n",
    "    Переходит на первую страницу поиска и \n",
    "    возвращает количество страниц\n",
    "    '''\n",
    "    params = {'q': word}\n",
    "    html = get_html(url, params=params)\n",
    "    soup = BeautifulSoup(html, 'html.parser')\n",
    "    num = int(re.findall(r'\\d+', soup.find_all('div', 'tm-pagination__page-group')[-1].text)[-1])\n",
    "    return num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a4190f66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "python 50\n",
      "mlflow 3\n",
      "airflow 11\n",
      "sql 50\n"
     ]
    }
   ],
   "source": [
    "for word in req:\n",
    "    print(word, get_num_pagins('https://habr.com/ru/search/', word))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7b54456",
   "metadata": {},
   "source": [
    "Похоже 50 - максимальное количество страниц, для некоторых слов станиц меньше.  \n",
    "Особенность в том, что если задать в запросе номер страницы превышающий максимум ответ будет __\"Страница не найдена\"__ и скрипт сломается. Это ограничение необходимо предусмотреть в итоговой функции."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9ec59122",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_article_details(url):\n",
    "    '''\n",
    "    Фунция принимает на вход url конкретной статьи.\n",
    "    Возвращает кортеж из двух значений:\n",
    "    - число баллов (может быть и отрицательным)\n",
    "    - текст новости, очищенный от пробельных символов\n",
    "    '''\n",
    "    html = get_html(url)\n",
    "    soup = BeautifulSoup(html, 'html.parser')\n",
    "    votes = soup.find('div', class_=\"tm-votes-meter tm-article-rating__votes-switcher\")\n",
    "    content = soup.find('div', class_='tm-article-body')\n",
    "    if votes is not None: # в некоторых статьях по этим тегам может ничего не вернуться\n",
    "        votes = int((votes).find('span').text) \n",
    "    if content is not None: # в некоторых статьях по этим тегам может ничего не вернуться\n",
    "        content = re.sub(r'\\s+',' ', content.get_text().strip()) # очищаем текст от переносов и табуляций\n",
    "    return (votes, content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "14702064",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 48.8 ms, sys: 3.6 ms, total: 52.4 ms\n",
      "Wall time: 252 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(13,\n",
       " 'В рамках развития программы Microsoft AI for Earth в компании анонсировали новые этапы информационно-технические этапы по сохранению биоразнообразия и природных экосистем нашей планеты. 15 апреля 2020 года Microsoft объявила, что скоро запустит открытую вычислительную платформу под названием «Планетарный компьютер» для сбора, хранения и анализа данных о состоянии Земли. Причем доступ к платформе как для загрузки данных, так и для получения информации о состоянии Земли, например, изменении размеров лесных массивов, оценки рисков затоплений, землетрясений и других природных катастроф, бесплатно получат исследователи, экологи, ученые, специалисты по охране природы и окружающего мира, некоммерческие организации и государственные учреждения всего мира. Microsoft AI for Earth — это часть глобального проекта компании под названием AI for Good, направленного на применение технологий искусственного интеллекта для борьбы с тремя глобальными проблемами: загрязнением окружающей среды (AI for Earth), природными катаклизмами и катастрофами (AI for Humanitarian action), а также поддержки людей с инвалидностью (AI for Accessibility). Фактически, «Планетарный компьютер» Microsoft будет основан на специально созданной Microsoft программной платформе с искусственным интеллектом на базе облака Microsoft Azure. ИИ планетарного компьютера будет использовать машинное обучение для анализа больших объемов данных из различных источников. Партнером Microsoft по разработке и развитию проекта «Планетарного компьютера объявлена компания Esri, которая является один из лидеров рынка геоинформационных систем. Ранее 20 марта 2020 представитель Microsoft на Хабре рассказал о том, над какими проектами работают специалисты программы Microsoft AI for Earth. Например, в холодных водах Аляски искусственный интеллект помогает исследователям в спасении животных, находящихся под угрозой исчезновения.')"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "get_article_details('https://habr.com/ru/news/t/497474/')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "286111ae",
   "metadata": {},
   "source": [
    "Читать статьи в таком формате не очень удобно. Извлеченный текст скорее подойдет для лингвистического анализа, векторизации, частоты упоминаний различных инструментов и терминов и т.п."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6b85933b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_content_pro(req, pages=4):\n",
    "    '''\n",
    "    Функия получения полной таблицы со статьями по списку поисковых слов\n",
    "    На вход принимает список слов и число станиц поиска\n",
    "    Внутри:\n",
    "    1) для каждого слова делает запрос к сайту\n",
    "    2) перебирает блоки со статьями на первой странице поиска\n",
    "    3) для каждой статьи собирает дату, название, ссылку, атора\n",
    "    4) переходит по сстылке статьи и забирает оттуда оценки и полный текст\n",
    "    5) добавляет конкретное поисковое слово\n",
    "    6) доавляет полученную строку к датафрейму\n",
    "    7) проходит по всем страницам, но не более чем передано в функцию\n",
    "    8) проходит в цикле по всем словам из переданного списка\n",
    "    9) удаляет дубли по столбцу со ссылками\n",
    "    '''\n",
    "    url = 'https://habr.com/ru/search/'\n",
    "    df = pd.DataFrame()\n",
    "    for word in req:\n",
    "        params = {'q': word}\n",
    "        current_word_pages = int(pages) # для текущего слова задаем кол-во страниц из функции\n",
    "        max_pages = get_num_pagins(url, word) # используем вспомогательную функцию для получения максимальных страниц\n",
    "        if current_word_pages > max_pages: # если кол-во страниц превышает максимальное \n",
    "            current_word_pages = max_pages # приравниваем к максимуму\n",
    "        else:\n",
    "            current_word_pages # в противном случае - сколько задали изначально\n",
    "        for page in range(1, current_word_pages+1):\n",
    "            html = get_html(url+f'page{page}/', params=params) # используем вспомогательную функцию для получения html\n",
    "            soup = BeautifulSoup(html, 'html.parser')\n",
    "            articles = soup.find('div', class_='tm-articles-list').find_all('div', 'tm-article-snippet')\n",
    "            for item in tqdm(articles):\n",
    "                sleep(0.1)\n",
    "                date = pd.to_datetime(item.find('time').get('datetime')).strftime('%Y-%m-%d %H:%M') \n",
    "                title = item.find('a', 'tm-article-snippet__title-link').text\n",
    "                link = 'https://habr.com' + item.find('a', 'tm-article-snippet__title-link').get('href')\n",
    "                votes = get_article_details(link)[0] # используем вспомогательную функцию для получения голосов\n",
    "                text = get_article_details(link)[1] # используем вспомогательную функцию для получения всего текста\n",
    "                author = item.find('span', 'tm-user-info__user').text.strip()\n",
    "                row = {'word':word, # собираем строку\n",
    "                       'date':date, \n",
    "                       'title':title, \n",
    "                       'link':link, \n",
    "                       'author':author, \n",
    "                       'votes':votes,\n",
    "                       'text':text}\n",
    "                df = df.append(row, ignore_index=True) # добавляем строку\n",
    "    df.drop_duplicates(subset='link', inplace=True) # удаляем дубли\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "31640d27",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [00:14<00:00,  1.41it/s]\n",
      "100%|██████████| 20/20 [00:22<00:00,  1.14s/it]\n",
      "100%|██████████| 20/20 [00:19<00:00,  1.03it/s]\n",
      "100%|██████████| 20/20 [00:27<00:00,  1.37s/it]\n",
      "100%|██████████| 20/20 [00:16<00:00,  1.23it/s]\n",
      "100%|██████████| 20/20 [00:21<00:00,  1.08s/it]\n",
      "100%|██████████| 20/20 [00:17<00:00,  1.16it/s]\n",
      "100%|██████████| 11/11 [00:09<00:00,  1.11it/s]\n",
      "100%|██████████| 20/20 [00:17<00:00,  1.14it/s]\n",
      "100%|██████████| 19/19 [00:15<00:00,  1.19it/s]\n",
      "100%|██████████| 20/20 [00:18<00:00,  1.07it/s]\n",
      "100%|██████████| 20/20 [00:16<00:00,  1.19it/s]\n",
      "100%|██████████| 20/20 [00:15<00:00,  1.26it/s]\n",
      "100%|██████████| 20/20 [00:19<00:00,  1.04it/s]\n",
      "100%|██████████| 20/20 [00:19<00:00,  1.03it/s]\n",
      "100%|██████████| 20/20 [00:18<00:00,  1.11it/s]\n",
      "100%|██████████| 20/20 [00:23<00:00,  1.17s/it]\n",
      "100%|██████████| 20/20 [00:20<00:00,  1.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 54.8 s, sys: 1.58 s, total: 56.4 s\n",
      "Wall time: 5min 47s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "content_pro = get_content_pro(req, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8273e68f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word</th>\n",
       "      <th>date</th>\n",
       "      <th>title</th>\n",
       "      <th>link</th>\n",
       "      <th>author</th>\n",
       "      <th>votes</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>python</td>\n",
       "      <td>2022-05-24 07:57</td>\n",
       "      <td>Дайджест Слёрма: тест на уровень кунг-фу по Py...</td>\n",
       "      <td>https://habr.com/ru/company/southbridge/news/t...</td>\n",
       "      <td>Lika_Chernigo</td>\n",
       "      <td>9.0</td>\n",
       "      <td>Сделали для вас подборку свежих статей и выгод...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>python</td>\n",
       "      <td>2022-05-18 15:33</td>\n",
       "      <td>Слёрм запускает 3-дневный интенсив по Python д...</td>\n",
       "      <td>https://habr.com/ru/company/southbridge/news/t...</td>\n",
       "      <td>edeshina</td>\n",
       "      <td>5.0</td>\n",
       "      <td>24-26 июня пройдёт онлайн-интенсив для инженер...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>204</th>\n",
       "      <td>airflow</td>\n",
       "      <td>2022-05-13 17:03</td>\n",
       "      <td>Кто такой Analytics Engineer – E2E-решение с и...</td>\n",
       "      <td>https://habr.com/ru/company/otus/blog/665642/</td>\n",
       "      <td>kzzzr</td>\n",
       "      <td>7.0</td>\n",
       "      <td>Привет! Меня зовут Артемий Козырь, и я Analyti...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>150</th>\n",
       "      <td>mlflow</td>\n",
       "      <td>2022-05-12 06:56</td>\n",
       "      <td>Как и для чего мы построили ML Space</td>\n",
       "      <td>https://habr.com/ru/company/sbercloud/blog/665...</td>\n",
       "      <td>SberCloud_Administrator</td>\n",
       "      <td>10.0</td>\n",
       "      <td>Речь пойдет о платформе для ML-разработки полн...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>209</th>\n",
       "      <td>airflow</td>\n",
       "      <td>2022-05-05 06:14</td>\n",
       "      <td>«Божественная комедия», или Девять кругов прог...</td>\n",
       "      <td>https://habr.com/ru/company/magnit/blog/664358/</td>\n",
       "      <td>He6puToCTb</td>\n",
       "      <td>8.0</td>\n",
       "      <td>Привет, Хабр! На связи команда направления про...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>149</th>\n",
       "      <td>mlflow</td>\n",
       "      <td>2022-05-04 08:44</td>\n",
       "      <td>Data-Science-процессы: Jupyter Notebook для пр...</td>\n",
       "      <td>https://habr.com/ru/company/vk/blog/662734/</td>\n",
       "      <td>Olga_Mokshina</td>\n",
       "      <td>37.0</td>\n",
       "      <td>Jovian Blues by ShootingStarLogBook Рефакторин...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>197</th>\n",
       "      <td>airflow</td>\n",
       "      <td>2022-04-25 08:43</td>\n",
       "      <td>Сравнение процессов ETL и ELT</td>\n",
       "      <td>https://habr.com/ru/post/662746/</td>\n",
       "      <td>LiMalk</td>\n",
       "      <td>0.0</td>\n",
       "      <td>ETL означает извлечение, преобразование и загр...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>106</th>\n",
       "      <td>mlflow</td>\n",
       "      <td>2022-04-23 05:48</td>\n",
       "      <td>Почему инструменты MLOps должны быть с открыты...</td>\n",
       "      <td>https://habr.com/ru/post/662519/</td>\n",
       "      <td>mnrozhkov</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Перевод статьи подготовлен совместно с Моргуно...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>python</td>\n",
       "      <td>2022-04-22 11:42</td>\n",
       "      <td>TechnoMeetsPython. Онлайн митап о Python-разра...</td>\n",
       "      <td>https://habr.com/ru/news/t/662437/</td>\n",
       "      <td>technokratiya</td>\n",
       "      <td>2.0</td>\n",
       "      <td>27 апреля в 18:00 собираем питонистов на YouTu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>232</th>\n",
       "      <td>airflow</td>\n",
       "      <td>2022-04-20 08:07</td>\n",
       "      <td>Четыре хитрости в работе с пайплайнами данных,...</td>\n",
       "      <td>https://habr.com/ru/company/vk/blog/659389/</td>\n",
       "      <td>Olga_Mokshina</td>\n",
       "      <td>13.0</td>\n",
       "      <td>Dust-n-Rust by Spiritofdarkness Команда разраб...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        word              date  \\\n",
       "22    python  2022-05-24 07:57   \n",
       "16    python  2022-05-18 15:33   \n",
       "204  airflow  2022-05-13 17:03   \n",
       "150   mlflow  2022-05-12 06:56   \n",
       "209  airflow  2022-05-05 06:14   \n",
       "149   mlflow  2022-05-04 08:44   \n",
       "197  airflow  2022-04-25 08:43   \n",
       "106   mlflow  2022-04-23 05:48   \n",
       "18    python  2022-04-22 11:42   \n",
       "232  airflow  2022-04-20 08:07   \n",
       "\n",
       "                                                 title  \\\n",
       "22   Дайджест Слёрма: тест на уровень кунг-фу по Py...   \n",
       "16   Слёрм запускает 3-дневный интенсив по Python д...   \n",
       "204  Кто такой Analytics Engineer – E2E-решение с и...   \n",
       "150               Как и для чего мы построили ML Space   \n",
       "209  «Божественная комедия», или Девять кругов прог...   \n",
       "149  Data-Science-процессы: Jupyter Notebook для пр...   \n",
       "197                      Сравнение процессов ETL и ELT   \n",
       "106  Почему инструменты MLOps должны быть с открыты...   \n",
       "18   TechnoMeetsPython. Онлайн митап о Python-разра...   \n",
       "232  Четыре хитрости в работе с пайплайнами данных,...   \n",
       "\n",
       "                                                  link  \\\n",
       "22   https://habr.com/ru/company/southbridge/news/t...   \n",
       "16   https://habr.com/ru/company/southbridge/news/t...   \n",
       "204      https://habr.com/ru/company/otus/blog/665642/   \n",
       "150  https://habr.com/ru/company/sbercloud/blog/665...   \n",
       "209    https://habr.com/ru/company/magnit/blog/664358/   \n",
       "149        https://habr.com/ru/company/vk/blog/662734/   \n",
       "197                   https://habr.com/ru/post/662746/   \n",
       "106                   https://habr.com/ru/post/662519/   \n",
       "18                  https://habr.com/ru/news/t/662437/   \n",
       "232        https://habr.com/ru/company/vk/blog/659389/   \n",
       "\n",
       "                      author  votes  \\\n",
       "22             Lika_Chernigo    9.0   \n",
       "16                  edeshina    5.0   \n",
       "204                    kzzzr    7.0   \n",
       "150  SberCloud_Administrator   10.0   \n",
       "209               He6puToCTb    8.0   \n",
       "149            Olga_Mokshina   37.0   \n",
       "197                   LiMalk    0.0   \n",
       "106                mnrozhkov    1.0   \n",
       "18             technokratiya    2.0   \n",
       "232            Olga_Mokshina   13.0   \n",
       "\n",
       "                                                  text  \n",
       "22   Сделали для вас подборку свежих статей и выгод...  \n",
       "16   24-26 июня пройдёт онлайн-интенсив для инженер...  \n",
       "204  Привет! Меня зовут Артемий Козырь, и я Analyti...  \n",
       "150  Речь пойдет о платформе для ML-разработки полн...  \n",
       "209  Привет, Хабр! На связи команда направления про...  \n",
       "149  Jovian Blues by ShootingStarLogBook Рефакторин...  \n",
       "197  ETL означает извлечение, преобразование и загр...  \n",
       "106  Перевод статьи подготовлен совместно с Моргуно...  \n",
       "18   27 апреля в 18:00 собираем питонистов на YouTu...  \n",
       "232  Dust-n-Rust by Spiritofdarkness Команда разраб...  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "content_pro.sort_values(by='date', ascending=False).head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56907ec7",
   "metadata": {},
   "source": [
    "Для одного слова на страницу требуется примерно 17 секунд. По замыслу скрипт стоит обернуть в питоновский файл.py и запускать по расписанию ночью. Все 50 страниц поиска должны выгрузиться за примерно 15-16 мин, следовательно 4 слова за 1 час, следовательно за 4 часа можно обработать 16 слов. Если понадобится больше слов, можно ограничиться 4-5 страницами."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3321d985",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>link</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>word</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>airflow</th>\n",
       "      <td>92</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mlflow</th>\n",
       "      <td>51</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>python</th>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sql</th>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         link\n",
       "word         \n",
       "airflow    92\n",
       "mlflow     51\n",
       "python    100\n",
       "sql       100"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "content_pro.groupby('word').agg({'link':'count'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "34b56a38",
   "metadata": {},
   "outputs": [],
   "source": [
    "req_2 = ['airflow']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "66d3f365",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [00:17<00:00,  1.13it/s]\n",
      "100%|██████████| 19/19 [00:15<00:00,  1.26it/s]\n",
      "100%|██████████| 20/20 [00:17<00:00,  1.18it/s]\n",
      "100%|██████████| 20/20 [00:17<00:00,  1.13it/s]\n",
      "100%|██████████| 20/20 [00:18<00:00,  1.07it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 14.4 s, sys: 482 ms, total: 14.9 s\n",
      "Wall time: 1min 29s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>link</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>word</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>airflow</th>\n",
       "      <td>99</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         link\n",
       "word         \n",
       "airflow    99"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "content_pro_airflow = get_content_pro(req_2, 5)\n",
    "content_pro_airflow.groupby('word').agg({'link':'count'})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f727ffe",
   "metadata": {},
   "source": [
    "Все логично.  \n",
    "- У слов __python__ и __sql__ собрано по 5 страниц с 20 статьями - всего по 100. \n",
    "- У слова __mlflow__ 3 страницы 20+20+11=51 статья\n",
    "- У слова __airflow__ тоже 5 страниц. всего 99 статей, но 7 удалились как дубли в смежных запросах"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
