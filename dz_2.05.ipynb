{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e589382b",
   "metadata": {},
   "source": [
    "### Домашнее задание №5 к лекции \"Основы веб-скрапинга\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ed03b14e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from time import sleep\n",
    "from datetime import datetime\n",
    "import re\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f744adcc",
   "metadata": {},
   "source": [
    "#### Задание 1\n",
    "Вам необходимо написать функцию, которая будет основана на поиске по сайту http://habr.com. Функция в качестве параметра должна принимать список запросов для поиска (например, ['python', 'анализ данных']) и на основе материалов, попавших в результаты поиска по каждому запросу, возвращать датафрейм вида:\n",
    "\n",
    "<дата> - <заголовок> - <ссылка на материал>\n",
    "В рамках задания предполагается работа только с одной (первой) страницей результатов поисковой выдачи для каждого запроса. Материалы в датафрейме не должны дублироваться, если они попадали в результаты поиска для нескольких запросов из списка."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e755d365",
   "metadata": {},
   "outputs": [],
   "source": [
    "req = ['большие данные', 'python', 'sql', 'big data']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "629c7d51",
   "metadata": {},
   "source": [
    "Так как HTML извлекать придется во каждом случае, создадим для этого вспомогательную функцию."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4dcc7c66",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_html(url, params=None):\n",
    "    ''' \n",
    "    Функция для получения HTML кода из url, \n",
    "    в слачае 200 ответа сайта\n",
    "    '''\n",
    "    html = requests.get(url, params=params)\n",
    "    if html.status_code == 200:\n",
    "        return html.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "576b43c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_content_lite(req):\n",
    "    '''\n",
    "    Функия получения короткой таблицы со статьями по списку поисковых слов\n",
    "    На вход принимает список\n",
    "    Внутри, \n",
    "    1) для каждого слова делает запрос к сайту\n",
    "    2) перебирает блоки со статьями на первой странице поиска\n",
    "    3) для каждой статьи собирает дату, название, ссылку, атора\n",
    "    4) добавляет конкретное поисковое слово\n",
    "    5) доавляет полученную строку к датафрейму\n",
    "    6) пробегает циклом по всем словам из переданного списка\n",
    "    7) удаляет дубли по столбцу со ссылками\n",
    "    работает только для первой страницы поиска\n",
    "    '''\n",
    "    url = 'https://habr.com/ru/search/'\n",
    "    data = pd.DataFrame()\n",
    "    for word in req:\n",
    "        params = {'q': word}\n",
    "        html = get_html(url, params=params) # используем вспомогательную функцию для получения html\n",
    "        soup = BeautifulSoup(html, 'html.parser')\n",
    "        articles = soup.find('div', class_='tm-articles-list').find_all('div', 'tm-article-snippet')\n",
    "        for item in tqdm(articles):\n",
    "            sleep(0.1)\n",
    "            date = pd.to_datetime(item.find('time').get('datetime')).strftime('%Y-%m-%d %H:%M') \n",
    "            title = item.find('a', 'tm-article-snippet__title-link').text\n",
    "            link = 'https://habr.com' + item.find('a', 'tm-article-snippet__title-link').get('href')\n",
    "            author = item.find('span', 'tm-user-info__user').text.strip()\n",
    "            row = {'word':word, \n",
    "                   'date':date, \n",
    "                   'title':title, \n",
    "                   'link':link, \n",
    "                   'author':author}\n",
    "            data = data.append(row, ignore_index=True)\n",
    "    data.drop_duplicates(subset='link', inplace=True)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ce1b6679",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 19/19 [00:02<00:00,  8.79it/s]\n",
      "100%|██████████| 20/20 [00:02<00:00,  8.77it/s]\n",
      "100%|██████████| 20/20 [00:02<00:00,  8.76it/s]\n",
      "100%|██████████| 19/19 [00:02<00:00,  8.78it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.13 s, sys: 91.8 ms, total: 1.23 s\n",
      "Wall time: 11.8 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "content_lite = get_content_lite(req)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bcecd7c7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word</th>\n",
       "      <th>date</th>\n",
       "      <th>title</th>\n",
       "      <th>link</th>\n",
       "      <th>author</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>sql</td>\n",
       "      <td>2022-05-20 11:19</td>\n",
       "      <td>Do it yourself: JIT компиляция SQL в Tarantool</td>\n",
       "      <td>https://habr.com/ru/company/vk/blog/666370/</td>\n",
       "      <td>CuriousGeorgiy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>python</td>\n",
       "      <td>2022-05-18 15:33</td>\n",
       "      <td>Слёрм запускает 3-дневный интенсив по Python д...</td>\n",
       "      <td>https://habr.com/ru/company/southbridge/news/t...</td>\n",
       "      <td>edeshina</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74</th>\n",
       "      <td>big data</td>\n",
       "      <td>2022-05-04 08:44</td>\n",
       "      <td>Data-Science-процессы: Jupyter Notebook для пр...</td>\n",
       "      <td>https://habr.com/ru/company/vk/blog/662734/</td>\n",
       "      <td>Olga_Mokshina</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>python</td>\n",
       "      <td>2022-04-22 11:42</td>\n",
       "      <td>TechnoMeetsPython. Онлайн митап о Python-разра...</td>\n",
       "      <td>https://habr.com/ru/news/t/662437/</td>\n",
       "      <td>technokratiya</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>большие данные</td>\n",
       "      <td>2022-04-07 14:23</td>\n",
       "      <td>17 лучших инструментов и технологий для работы...</td>\n",
       "      <td>https://habr.com/ru/company/otus/blog/659657/</td>\n",
       "      <td>kmoseenk</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>sql</td>\n",
       "      <td>2022-04-07 08:56</td>\n",
       "      <td>Яндекс Практикум запускает курс «SQL для работ...</td>\n",
       "      <td>https://habr.com/ru/company/yandex_praktikum/n...</td>\n",
       "      <td>eshulyndina</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>python</td>\n",
       "      <td>2022-03-18 15:31</td>\n",
       "      <td>24 марта Слёрм проведёт открытый урок «Первый ...</td>\n",
       "      <td>https://habr.com/ru/company/southbridge/news/t...</td>\n",
       "      <td>edeshina</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62</th>\n",
       "      <td>big data</td>\n",
       "      <td>2022-03-10 08:30</td>\n",
       "      <td>10—24 марта: Big Data Dev Week от билайна</td>\n",
       "      <td>https://habr.com/ru/company/beeline/news/t/654...</td>\n",
       "      <td>Bee_brightside</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>python</td>\n",
       "      <td>2022-03-08 09:13</td>\n",
       "      <td>Вышел мартовский релиз расширения Python для V...</td>\n",
       "      <td>https://habr.com/ru/news/t/654707/</td>\n",
       "      <td>maybe_elf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75</th>\n",
       "      <td>big data</td>\n",
       "      <td>2022-02-18 12:51</td>\n",
       "      <td>Citymobil Data Meetup №7</td>\n",
       "      <td>https://habr.com/ru/company/citymobil/news/t/6...</td>\n",
       "      <td>leleles</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              word              date  \\\n",
       "39             sql  2022-05-20 11:19   \n",
       "35          python  2022-05-18 15:33   \n",
       "74        big data  2022-05-04 08:44   \n",
       "37          python  2022-04-22 11:42   \n",
       "13  большие данные  2022-04-07 14:23   \n",
       "41             sql  2022-04-07 08:56   \n",
       "36          python  2022-03-18 15:31   \n",
       "62        big data  2022-03-10 08:30   \n",
       "25          python  2022-03-08 09:13   \n",
       "75        big data  2022-02-18 12:51   \n",
       "\n",
       "                                                title  \\\n",
       "39     Do it yourself: JIT компиляция SQL в Tarantool   \n",
       "35  Слёрм запускает 3-дневный интенсив по Python д...   \n",
       "74  Data-Science-процессы: Jupyter Notebook для пр...   \n",
       "37  TechnoMeetsPython. Онлайн митап о Python-разра...   \n",
       "13  17 лучших инструментов и технологий для работы...   \n",
       "41  Яндекс Практикум запускает курс «SQL для работ...   \n",
       "36  24 марта Слёрм проведёт открытый урок «Первый ...   \n",
       "62          10—24 марта: Big Data Dev Week от билайна   \n",
       "25  Вышел мартовский релиз расширения Python для V...   \n",
       "75                           Citymobil Data Meetup №7   \n",
       "\n",
       "                                                 link          author  \n",
       "39        https://habr.com/ru/company/vk/blog/666370/  CuriousGeorgiy  \n",
       "35  https://habr.com/ru/company/southbridge/news/t...        edeshina  \n",
       "74        https://habr.com/ru/company/vk/blog/662734/   Olga_Mokshina  \n",
       "37                 https://habr.com/ru/news/t/662437/   technokratiya  \n",
       "13      https://habr.com/ru/company/otus/blog/659657/        kmoseenk  \n",
       "41  https://habr.com/ru/company/yandex_praktikum/n...     eshulyndina  \n",
       "36  https://habr.com/ru/company/southbridge/news/t...        edeshina  \n",
       "62  https://habr.com/ru/company/beeline/news/t/654...  Bee_brightside  \n",
       "25                 https://habr.com/ru/news/t/654707/       maybe_elf  \n",
       "75  https://habr.com/ru/company/citymobil/news/t/6...         leleles  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "content_lite.sort_values(by='date', ascending=False).head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14fe2942",
   "metadata": {},
   "source": [
    "Для каждого поискового слова скрипт срабатывает за 2 секунды"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cfb1a27",
   "metadata": {},
   "source": [
    "#### Задание 2  \n",
    "Функция из обязательной части задания должна быть расширена следующим образом:\n",
    "\n",
    "кроме списка ключевых слов для поиска необходимо объявить параметр с количеством страниц поисковой выдачи. Т.е. при передаче в функцию аргумента 4 необходимо получить материалы с первых 4 страниц результатов;\n",
    "в датафрейме должны быть столбцы с полным текстом найденных материалов и количеством лайков:\n",
    "<дата> - <заголовок> - <ссылка на материал> - <текст материала> - <количество лайков>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4f5cff18",
   "metadata": {},
   "outputs": [],
   "source": [
    "req = ['большие данные', 'python', 'big data']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2f6ab26",
   "metadata": {},
   "source": [
    "Добавим еще две вспомогательные функции. \n",
    "1) Для определения количества страниц поиска (это максимальное количество, дольше поиск работать не будет)  \n",
    "2) Для получения полного текста статьи и количества оценок"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5898831b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_num_pagins(url, word):\n",
    "    '''Фукнция принимает на вход url и поисковое слово.\n",
    "    Переходит на первую страницу поиска и \n",
    "    возвращает количе страниц\n",
    "    '''\n",
    "    params = {'q': word}\n",
    "    html = get_html(url, params=params)\n",
    "    soup = BeautifulSoup(html, 'html.parser')\n",
    "    num = int(soup.find_all('div', 'tm-pagination__page-group')[-1].get_text())\n",
    "    return num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e35b0b92",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_num_pagins('https://habr.com/ru/search/', 'python')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fd8e1a85",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_article_details(url):\n",
    "    '''\n",
    "    Фунция принимет на вход url конкретной статьи.\n",
    "    Возвращает кортеж из двух значений:\n",
    "    - число баллов (может быть и отрицательным)\n",
    "    - текст новости и накопленное \n",
    "    '''\n",
    "    html = get_html(url)\n",
    "    soup = BeautifulSoup(html, 'html.parser')\n",
    "    votes = soup.find('div', class_=\"tm-votes-meter tm-article-rating__votes-switcher\").find('span').text    \n",
    "    text = soup.find('div', class_='tm-article-body').get_text().strip()\n",
    "    content = ''.join(re.findall(r'\\S*\\ ', text, flags=re.IGNORECASE)) # очищаем текст от переносов и табуляций\n",
    "    return (int(votes), content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "14702064",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(13,\n",
       " 'В рамках развития программы Microsoft AI for Earth в компании анонсировали новые этапы информационно-технические этапы по сохранению биоразнообразия и природных экосистем нашей планеты. 15 апреля 2020 года Microsoft объявила, что скоро запустит открытую вычислительную платформу под названием «Планетарный компьютер» для сбора, хранения и анализа данных о состоянии Земли. Причем доступ к платформе как для загрузки данных, так и для получения информации о состоянии Земли, например, изменении размеров лесных массивов, оценки рисков затоплений, землетрясений и других природных катастроф, бесплатно получат исследователи, экологи, ученые, специалисты по охране природы и окружающего мира, некоммерческие организации и государственные учреждения всего Microsoft AI for Earth — это часть глобального проекта компании под названием AI for Good, направленного на применение технологий искусственного интеллекта для борьбы с тремя глобальными проблемами: загрязнением окружающей среды (AI for Earth), природными катаклизмами и катастрофами (AI for Humanitarian action), а также поддержки людей с инвалидностью (AI for Фактически, «Планетарный компьютер» Microsoft будет основан на специально созданной Microsoft программной платформе с искусственным интеллектом на базе облака Microsoft Azure. ИИ планетарного компьютера будет использовать машинное обучение для анализа больших объемов данных из различных источников. Партнером Microsoft по разработке и развитию проекта «Планетарного компьютера объявлена компания Esri, которая является один из лидеров рынка геоинформационных Ранее 20 марта 2020 представитель Microsoft на Хабре рассказал о том, над какими проектами работают специалисты программы Microsoft AI for Earth. Например, в холодных водах Аляски искусственный интеллект помогает исследователям в спасении животных, находящихся под угрозой ')"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_article_details('https://habr.com/ru/news/t/497474/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6b85933b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_content_pro(req, pages=4):\n",
    "    '''\n",
    "    Функия получения расширенной таблицы со статьями по списку поисковых слов\n",
    "    На вход принимает список и количество страниц поиска\n",
    "    Внутри, \n",
    "    1) для каждого слова делает запрос к сайту\n",
    "    2) перебирает блоки со статьями на первой странице поиска\n",
    "    3) для каждой статьи собирает дату, название, ссылку, атора\n",
    "    4) переходит по сстылке статьи и забирает оттуда оценки и полный текст\n",
    "    5) добавляет конкретное поисковое слово\n",
    "    6) доавляет полученную строку к датафрейму\n",
    "    7) пробегает по всем страницам. если передать больше чем надо, ничего не вернется\n",
    "    8) пробегает циклом по всем словам из переданного списка\n",
    "    9) удаляет дубли по столбцу со ссылками\n",
    "    '''\n",
    "    url = 'https://habr.com/ru/search/'\n",
    "    data = pd.DataFrame()\n",
    "    for word in req:\n",
    "        params = {'q': word}\n",
    "        if pages <= get_num_pagins(url, word):\n",
    "            for page in range(1, pages+1):\n",
    "                html = get_html(url+f'page{page}/', params=params)\n",
    "                soup = BeautifulSoup(html, 'html.parser')\n",
    "                articles = soup.find('div', class_='tm-articles-list').find_all('div', 'tm-article-snippet')\n",
    "                for item in tqdm(articles):\n",
    "                    sleep(0.1)\n",
    "                    date = pd.to_datetime(item.find('time').get('datetime')).strftime('%Y-%m-%d %H:%M') \n",
    "                    title = item.find('a', 'tm-article-snippet__title-link').text\n",
    "                    link = 'https://habr.com' + item.find('a', 'tm-article-snippet__title-link').get('href')\n",
    "                    try:\n",
    "                        votes = get_article_details(link)[0]\n",
    "                    except:\n",
    "                        votes = None\n",
    "                    try:\n",
    "                        text = get_article_details(link)[1]\n",
    "                    except:\n",
    "                        text = None\n",
    "                    author = item.find('span', 'tm-user-info__user').text.strip()\n",
    "                    row = {'word':word, \n",
    "                           'date':date, \n",
    "                           'title':title, \n",
    "                           'link':link, \n",
    "                           'author':author, \n",
    "                           'votes':votes,\n",
    "                           'text':text}\n",
    "                    data = data.append(row, ignore_index=True)\n",
    "    data.drop_duplicates(subset='link', inplace=True)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "31640d27",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 19/19 [00:18<00:00,  1.02it/s]\n",
      "100%|██████████| 20/20 [00:16<00:00,  1.19it/s]\n",
      "100%|██████████| 19/19 [00:15<00:00,  1.22it/s]\n",
      "100%|██████████| 20/20 [00:16<00:00,  1.22it/s]\n",
      "100%|██████████| 20/20 [00:16<00:00,  1.18it/s]\n",
      "100%|██████████| 20/20 [00:15<00:00,  1.28it/s]\n",
      "100%|██████████| 20/20 [00:16<00:00,  1.24it/s]\n",
      "100%|██████████| 20/20 [00:17<00:00,  1.13it/s]\n",
      "100%|██████████| 20/20 [00:17<00:00,  1.13it/s]\n",
      "100%|██████████| 20/20 [00:18<00:00,  1.10it/s]\n",
      "100%|██████████| 19/19 [00:14<00:00,  1.31it/s]\n",
      "100%|██████████| 19/19 [00:17<00:00,  1.11it/s]\n",
      "100%|██████████| 20/20 [00:16<00:00,  1.23it/s]\n",
      "100%|██████████| 20/20 [00:17<00:00,  1.12it/s]\n",
      "100%|██████████| 20/20 [00:18<00:00,  1.09it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 41.8 s, sys: 1.22 s, total: 43 s\n",
      "Wall time: 4min 26s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "content_pro = get_content_pro(req, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8273e68f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word</th>\n",
       "      <th>date</th>\n",
       "      <th>title</th>\n",
       "      <th>link</th>\n",
       "      <th>author</th>\n",
       "      <th>votes</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>127</th>\n",
       "      <td>python</td>\n",
       "      <td>2022-05-27 10:49</td>\n",
       "      <td>«Ваша сезонность, сэр!»: ищем тренд и прогнози...</td>\n",
       "      <td>https://habr.com/ru/post/668186/</td>\n",
       "      <td>kaza4ka</td>\n",
       "      <td>6.0</td>\n",
       "      <td>Как вы можете помнить по первой статье \"Маркет...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>126</th>\n",
       "      <td>python</td>\n",
       "      <td>2022-05-27 07:48</td>\n",
       "      <td>Работа с фреймворками Python: преимущества и п...</td>\n",
       "      <td>https://habr.com/ru/company/southbridge/blog/6...</td>\n",
       "      <td>edeshina</td>\n",
       "      <td>10.0</td>\n",
       "      <td>Фреймворки помогают ускорить разработку и сдел...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>122</th>\n",
       "      <td>python</td>\n",
       "      <td>2022-05-24 07:57</td>\n",
       "      <td>Дайджест Слёрма: тест на уровень кунг-фу по Py...</td>\n",
       "      <td>https://habr.com/ru/company/southbridge/news/t...</td>\n",
       "      <td>Lika_Chernigo</td>\n",
       "      <td>9.0</td>\n",
       "      <td>Сделали для вас подборку свежих статей и выгод...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>114</th>\n",
       "      <td>python</td>\n",
       "      <td>2022-05-18 15:33</td>\n",
       "      <td>Слёрм запускает 3-дневный интенсив по Python д...</td>\n",
       "      <td>https://habr.com/ru/company/southbridge/news/t...</td>\n",
       "      <td>edeshina</td>\n",
       "      <td>5.0</td>\n",
       "      <td>24-26 июня пройдёт онлайн-интенсив для инженер...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86</th>\n",
       "      <td>большие данные</td>\n",
       "      <td>2022-05-06 08:00</td>\n",
       "      <td>Конференция Data Fusion: большие спецы по боль...</td>\n",
       "      <td>https://habr.com/ru/company/vtb/blog/664596/</td>\n",
       "      <td>VTB</td>\n",
       "      <td>3.0</td>\n",
       "      <td>В 2022 году «бигдатой» никого не удивишь. Эта ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>213</th>\n",
       "      <td>big data</td>\n",
       "      <td>2022-05-04 08:44</td>\n",
       "      <td>Data-Science-процессы: Jupyter Notebook для пр...</td>\n",
       "      <td>https://habr.com/ru/company/vk/blog/662734/</td>\n",
       "      <td>Olga_Mokshina</td>\n",
       "      <td>37.0</td>\n",
       "      <td>Jovian Blues by Рефакторинг написанного в Note...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>290</th>\n",
       "      <td>big data</td>\n",
       "      <td>2022-04-28 12:38</td>\n",
       "      <td>Big Data и логистика: чем большие данные полез...</td>\n",
       "      <td>https://habr.com/ru/post/663470/</td>\n",
       "      <td>ibm</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>Привет, Хабр! Мы – сервис для оптимизации внут...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>116</th>\n",
       "      <td>python</td>\n",
       "      <td>2022-04-22 11:42</td>\n",
       "      <td>TechnoMeetsPython. Онлайн митап о Python-разра...</td>\n",
       "      <td>https://habr.com/ru/news/t/662437/</td>\n",
       "      <td>technokratiya</td>\n",
       "      <td>2.0</td>\n",
       "      <td>27 апреля в 18:00 собираем питонистов на YouTu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>129</th>\n",
       "      <td>python</td>\n",
       "      <td>2022-04-18 03:29</td>\n",
       "      <td>Онлайн-митап от руководителя практики Python U...</td>\n",
       "      <td>https://habr.com/ru/company/usetech/news/t/661...</td>\n",
       "      <td>Usetech</td>\n",
       "      <td>0.0</td>\n",
       "      <td>В конце марта Мстислав Казаков, руководитель п...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>большие данные</td>\n",
       "      <td>2022-04-07 14:23</td>\n",
       "      <td>17 лучших инструментов и технологий для работы...</td>\n",
       "      <td>https://habr.com/ru/company/otus/blog/659657/</td>\n",
       "      <td>kmoseenk</td>\n",
       "      <td>7.0</td>\n",
       "      <td>Мир больших данных становится только еще больш...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               word              date  \\\n",
       "127          python  2022-05-27 10:49   \n",
       "126          python  2022-05-27 07:48   \n",
       "122          python  2022-05-24 07:57   \n",
       "114          python  2022-05-18 15:33   \n",
       "86   большие данные  2022-05-06 08:00   \n",
       "213        big data  2022-05-04 08:44   \n",
       "290        big data  2022-04-28 12:38   \n",
       "116          python  2022-04-22 11:42   \n",
       "129          python  2022-04-18 03:29   \n",
       "13   большие данные  2022-04-07 14:23   \n",
       "\n",
       "                                                 title  \\\n",
       "127  «Ваша сезонность, сэр!»: ищем тренд и прогнози...   \n",
       "126  Работа с фреймворками Python: преимущества и п...   \n",
       "122  Дайджест Слёрма: тест на уровень кунг-фу по Py...   \n",
       "114  Слёрм запускает 3-дневный интенсив по Python д...   \n",
       "86   Конференция Data Fusion: большие спецы по боль...   \n",
       "213  Data-Science-процессы: Jupyter Notebook для пр...   \n",
       "290  Big Data и логистика: чем большие данные полез...   \n",
       "116  TechnoMeetsPython. Онлайн митап о Python-разра...   \n",
       "129  Онлайн-митап от руководителя практики Python U...   \n",
       "13   17 лучших инструментов и технологий для работы...   \n",
       "\n",
       "                                                  link         author  votes  \\\n",
       "127                   https://habr.com/ru/post/668186/        kaza4ka    6.0   \n",
       "126  https://habr.com/ru/company/southbridge/blog/6...       edeshina   10.0   \n",
       "122  https://habr.com/ru/company/southbridge/news/t...  Lika_Chernigo    9.0   \n",
       "114  https://habr.com/ru/company/southbridge/news/t...       edeshina    5.0   \n",
       "86        https://habr.com/ru/company/vtb/blog/664596/            VTB    3.0   \n",
       "213        https://habr.com/ru/company/vk/blog/662734/  Olga_Mokshina   37.0   \n",
       "290                   https://habr.com/ru/post/663470/            ibm   -1.0   \n",
       "116                 https://habr.com/ru/news/t/662437/  technokratiya    2.0   \n",
       "129  https://habr.com/ru/company/usetech/news/t/661...        Usetech    0.0   \n",
       "13       https://habr.com/ru/company/otus/blog/659657/       kmoseenk    7.0   \n",
       "\n",
       "                                                  text  \n",
       "127  Как вы можете помнить по первой статье \"Маркет...  \n",
       "126  Фреймворки помогают ускорить разработку и сдел...  \n",
       "122  Сделали для вас подборку свежих статей и выгод...  \n",
       "114  24-26 июня пройдёт онлайн-интенсив для инженер...  \n",
       "86   В 2022 году «бигдатой» никого не удивишь. Эта ...  \n",
       "213  Jovian Blues by Рефакторинг написанного в Note...  \n",
       "290  Привет, Хабр! Мы – сервис для оптимизации внут...  \n",
       "116  27 апреля в 18:00 собираем питонистов на YouTu...  \n",
       "129  В конце марта Мстислав Казаков, руководитель п...  \n",
       "13   Мир больших данных становится только еще больш...  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "content_pro.sort_values(by='date', ascending=False).head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57737462",
   "metadata": {},
   "source": [
    "По логике скрипт лучше запускать по расписанию ночью. 50 страниц поиска должны выгрузиться за примерно 14-15 мин, следовательно 4 слова за 1 час, следовательно за 4 часа можно обработать 16 слов. Результат можно сохранить в файл и отправить на почту"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
