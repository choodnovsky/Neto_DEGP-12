{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e589382b",
   "metadata": {},
   "source": [
    "### Домашнее задание №5 к лекции \"Основы веб-скрапинга\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ed03b14e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from time import sleep\n",
    "from datetime import datetime\n",
    "import re\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f744adcc",
   "metadata": {},
   "source": [
    "#### Задание 1\n",
    "Вам необходимо написать функцию, которая будет основана на поиске по сайту http://habr.com. Функция в качестве параметра должна принимать список запросов для поиска (например, ['python', 'анализ данных']) и на основе материалов, попавших в результаты поиска по каждому запросу, возвращать датафрейм вида:\n",
    "\n",
    "<дата> - <заголовок> - <ссылка на материал>\n",
    "В рамках задания предполагается работа только с одной (первой) страницей результатов поисковой выдачи для каждого запроса. Материалы в датафрейме не должны дублироваться, если они попадали в результаты поиска для нескольких запросов из списка."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e755d365",
   "metadata": {},
   "outputs": [],
   "source": [
    "req = ['большие данные', 'python', 'sql', 'big data']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4b5ab9e",
   "metadata": {},
   "source": [
    "Так как HTML извлекать придется во каждом случае, создадим для этого вспомогательную функцию."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4dcc7c66",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_html(url, params=None):\n",
    "    ''' \n",
    "    Функция для получения HTML кода из url в случае 200 ответа сайта.\n",
    "    В противном - ничего\n",
    "    '''\n",
    "    html = requests.get(url, params=params)\n",
    "    if html.status_code == 200:\n",
    "        return html.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "576b43c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_content_lite(req):\n",
    "    '''\n",
    "    Функия получения короткой таблицы со статьями по списку поисковых слов\n",
    "    На вход принимает список\n",
    "    Внутри, \n",
    "    1) для каждого слова делает запрос к сайту\n",
    "    2) перебирает блоки со статьями на первой странице поиска\n",
    "    3) для каждой статьи собирает дату, название, ссылку, атора\n",
    "    4) добавляет конкретное поисковое слово\n",
    "    5) доавляет полученную строку к датафрейму\n",
    "    6) пробегает циклом по всем словам из переданного списка\n",
    "    7) удаляет дубли по столбцу со ссылками\n",
    "    работает только для первой страницы поиска\n",
    "    '''\n",
    "    url = 'https://habr.com/ru/search/'\n",
    "    data = pd.DataFrame()\n",
    "    for word in req:\n",
    "        params = {'q': word}\n",
    "        html = get_html(url, params=params) # используем вспомогательную функцию для получения html\n",
    "        soup = BeautifulSoup(html, 'html.parser')\n",
    "        articles = soup.find('div', class_='tm-articles-list').find_all('div', 'tm-article-snippet')\n",
    "        for item in tqdm(articles):\n",
    "            sleep(0.1)\n",
    "            date = pd.to_datetime(item.find('time').get('datetime')).strftime('%Y-%m-%d %H:%M') \n",
    "            title = item.find('a', 'tm-article-snippet__title-link').text\n",
    "            link = 'https://habr.com' + item.find('a', 'tm-article-snippet__title-link').get('href')\n",
    "            author = item.find('span', 'tm-user-info__user').text.strip()\n",
    "            row = {'word':word, \n",
    "                   'date':date, \n",
    "                   'title':title, \n",
    "                   'link':link, \n",
    "                   'author':author}\n",
    "            data = data.append(row, ignore_index=True)\n",
    "    data.drop_duplicates(subset='link', inplace=True)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ce1b6679",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 19/19 [00:02<00:00,  8.80it/s]\n",
      "100%|██████████| 20/20 [00:02<00:00,  8.84it/s]\n",
      "100%|██████████| 20/20 [00:02<00:00,  8.82it/s]\n",
      "100%|██████████| 19/19 [00:02<00:00,  8.79it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.15 s, sys: 99.5 ms, total: 1.24 s\n",
      "Wall time: 12.6 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "content_lite = get_content_lite(req)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bcecd7c7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word</th>\n",
       "      <th>date</th>\n",
       "      <th>title</th>\n",
       "      <th>link</th>\n",
       "      <th>author</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>python</td>\n",
       "      <td>2022-05-18 15:33</td>\n",
       "      <td>Слёрм запускает 3-дневный интенсив по Python д...</td>\n",
       "      <td>https://habr.com/ru/company/southbridge/news/t...</td>\n",
       "      <td>edeshina</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>python</td>\n",
       "      <td>2022-04-22 11:42</td>\n",
       "      <td>TechnoMeetsPython. Онлайн митап о Python-разра...</td>\n",
       "      <td>https://habr.com/ru/news/t/662437/</td>\n",
       "      <td>technokratiya</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>большие данные</td>\n",
       "      <td>2022-04-07 14:23</td>\n",
       "      <td>17 лучших инструментов и технологий для работы...</td>\n",
       "      <td>https://habr.com/ru/company/otus/blog/659657/</td>\n",
       "      <td>kmoseenk</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>sql</td>\n",
       "      <td>2022-04-07 08:56</td>\n",
       "      <td>Яндекс Практикум запускает курс «SQL для работ...</td>\n",
       "      <td>https://habr.com/ru/company/yandex_praktikum/n...</td>\n",
       "      <td>eshulyndina</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>python</td>\n",
       "      <td>2022-03-18 15:31</td>\n",
       "      <td>24 марта Слёрм проведёт открытый урок «Первый ...</td>\n",
       "      <td>https://habr.com/ru/company/southbridge/news/t...</td>\n",
       "      <td>edeshina</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62</th>\n",
       "      <td>big data</td>\n",
       "      <td>2022-03-10 08:30</td>\n",
       "      <td>10—24 марта: Big Data Dev Week от билайна</td>\n",
       "      <td>https://habr.com/ru/company/beeline/news/t/654...</td>\n",
       "      <td>Bee_brightside</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>python</td>\n",
       "      <td>2022-03-08 09:13</td>\n",
       "      <td>Вышел мартовский релиз расширения Python для V...</td>\n",
       "      <td>https://habr.com/ru/news/t/654707/</td>\n",
       "      <td>maybe_elf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74</th>\n",
       "      <td>big data</td>\n",
       "      <td>2022-02-18 12:51</td>\n",
       "      <td>Citymobil Data Meetup №7</td>\n",
       "      <td>https://habr.com/ru/company/citymobil/news/t/6...</td>\n",
       "      <td>leleles</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>python</td>\n",
       "      <td>2022-01-20 15:37</td>\n",
       "      <td>Курс «Python для инженеров». Старт 3 потока 31...</td>\n",
       "      <td>https://habr.com/ru/company/southbridge/news/t...</td>\n",
       "      <td>Hedgehog_art</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63</th>\n",
       "      <td>big data</td>\n",
       "      <td>2022-01-20 11:02</td>\n",
       "      <td>Citymobil Data Meetup №6</td>\n",
       "      <td>https://habr.com/ru/company/citymobil/news/t/6...</td>\n",
       "      <td>leleles</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              word              date  \\\n",
       "35          python  2022-05-18 15:33   \n",
       "37          python  2022-04-22 11:42   \n",
       "13  большие данные  2022-04-07 14:23   \n",
       "40             sql  2022-04-07 08:56   \n",
       "36          python  2022-03-18 15:31   \n",
       "62        big data  2022-03-10 08:30   \n",
       "25          python  2022-03-08 09:13   \n",
       "74        big data  2022-02-18 12:51   \n",
       "19          python  2022-01-20 15:37   \n",
       "63        big data  2022-01-20 11:02   \n",
       "\n",
       "                                                title  \\\n",
       "35  Слёрм запускает 3-дневный интенсив по Python д...   \n",
       "37  TechnoMeetsPython. Онлайн митап о Python-разра...   \n",
       "13  17 лучших инструментов и технологий для работы...   \n",
       "40  Яндекс Практикум запускает курс «SQL для работ...   \n",
       "36  24 марта Слёрм проведёт открытый урок «Первый ...   \n",
       "62          10—24 марта: Big Data Dev Week от билайна   \n",
       "25  Вышел мартовский релиз расширения Python для V...   \n",
       "74                           Citymobil Data Meetup №7   \n",
       "19  Курс «Python для инженеров». Старт 3 потока 31...   \n",
       "63                           Citymobil Data Meetup №6   \n",
       "\n",
       "                                                 link          author  \n",
       "35  https://habr.com/ru/company/southbridge/news/t...        edeshina  \n",
       "37                 https://habr.com/ru/news/t/662437/   technokratiya  \n",
       "13      https://habr.com/ru/company/otus/blog/659657/        kmoseenk  \n",
       "40  https://habr.com/ru/company/yandex_praktikum/n...     eshulyndina  \n",
       "36  https://habr.com/ru/company/southbridge/news/t...        edeshina  \n",
       "62  https://habr.com/ru/company/beeline/news/t/654...  Bee_brightside  \n",
       "25                 https://habr.com/ru/news/t/654707/       maybe_elf  \n",
       "74  https://habr.com/ru/company/citymobil/news/t/6...         leleles  \n",
       "19  https://habr.com/ru/company/southbridge/news/t...    Hedgehog_art  \n",
       "63  https://habr.com/ru/company/citymobil/news/t/6...         leleles  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "content_lite.sort_values(by='date', ascending=False).head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd7f3313",
   "metadata": {},
   "source": [
    "Для каждого поискового слова скрипт срабатывает за 2 секунды"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cfb1a27",
   "metadata": {},
   "source": [
    "#### Задание 2  \n",
    "Функция из обязательной части задания должна быть расширена следующим образом:\n",
    "\n",
    "кроме списка ключевых слов для поиска необходимо объявить параметр с количеством страниц поисковой выдачи. Т.е. при передаче в функцию аргумента 4 необходимо получить материалы с первых 4 страниц результатов;\n",
    "в датафрейме должны быть столбцы с полным текстом найденных материалов и количеством лайков:\n",
    "<дата> - <заголовок> - <ссылка на материал> - <текст материала> - <количество лайков>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4f5cff18",
   "metadata": {},
   "outputs": [],
   "source": [
    "req = ['большие данные', 'python', 'sql']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "673a25ed",
   "metadata": {},
   "source": [
    "Добавим еще две вспомогательные функции. \n",
    "1) Для определения количества страниц поиска (это максимальное количество, дольше поиск работать не будет)  \n",
    "2) Для получения полного текста статьи и количества оценок"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5898831b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_num_pagins(url, word):\n",
    "    '''Фукнция принимает на вход url и поисковое слово.\n",
    "    Переходит на первую страницу поиска и \n",
    "    возвращает количе страниц\n",
    "    '''\n",
    "    params = {'q': word}\n",
    "    html = get_html(url, params=params)\n",
    "    soup = BeautifulSoup(html, 'html.parser')\n",
    "    num = int(soup.find_all('div', 'tm-pagination__page-group')[-1].get_text())\n",
    "    return num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e35b0b92",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_num_pagins('https://habr.com/ru/search/', 'python')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1ccfefef",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_article_details(url):\n",
    "    '''\n",
    "    Фунция принимет на вход url конкретной статьи.\n",
    "    Возвращает кортеж из двух значений:\n",
    "    - число баллов (может быть и отрицательным)\n",
    "    - текст новости и накопленное \n",
    "    '''\n",
    "    html = get_html(url)\n",
    "    soup = BeautifulSoup(html, 'html.parser')\n",
    "    votes = soup.find('div', class_=\"tm-votes-meter tm-article-rating__votes-switcher\").find('span').text    \n",
    "    text = soup.find('div', class_='tm-article-body').get_text().strip()\n",
    "    content = re.sub(r'\\s+',' ', text) # очищаем текст от переносов и табуляций\n",
    "    return (int(votes), content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "14702064",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(13,\n",
       " 'В рамках развития программы Microsoft AI for Earth в компании анонсировали новые этапы информационно-технические этапы по сохранению биоразнообразия и природных экосистем нашей планеты. 15 апреля 2020 года Microsoft объявила, что скоро запустит открытую вычислительную платформу под названием «Планетарный компьютер» для сбора, хранения и анализа данных о состоянии Земли. Причем доступ к платформе как для загрузки данных, так и для получения информации о состоянии Земли, например, изменении размеров лесных массивов, оценки рисков затоплений, землетрясений и других природных катастроф, бесплатно получат исследователи, экологи, ученые, специалисты по охране природы и окружающего мира, некоммерческие организации и государственные учреждения всего мира. Microsoft AI for Earth — это часть глобального проекта компании под названием AI for Good, направленного на применение технологий искусственного интеллекта для борьбы с тремя глобальными проблемами: загрязнением окружающей среды (AI for Earth), природными катаклизмами и катастрофами (AI for Humanitarian action), а также поддержки людей с инвалидностью (AI for Accessibility). Фактически, «Планетарный компьютер» Microsoft будет основан на специально созданной Microsoft программной платформе с искусственным интеллектом на базе облака Microsoft Azure. ИИ планетарного компьютера будет использовать машинное обучение для анализа больших объемов данных из различных источников. Партнером Microsoft по разработке и развитию проекта «Планетарного компьютера объявлена компания Esri, которая является один из лидеров рынка геоинформационных систем. Ранее 20 марта 2020 представитель Microsoft на Хабре рассказал о том, над какими проектами работают специалисты программы Microsoft AI for Earth. Например, в холодных водах Аляски искусственный интеллект помогает исследователям в спасении животных, находящихся под угрозой исчезновения.')"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_article_details('https://habr.com/ru/news/t/497474/')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "286111ae",
   "metadata": {},
   "source": [
    "Читать статьи в таком формате не очень удобно. Извлеченный текст скорее подойдет для лингвистического анализа, векторизации, частоты упоминаний различных инструментов и терминов и т.п."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6b85933b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_content_pro(req, pages=4):\n",
    "    '''\n",
    "    Функия получения расширенной таблицы со статьями по списку поисковых слов\n",
    "    На вход принимает список и количество страниц поиска\n",
    "    Внутри, \n",
    "    1) для каждого слова делает запрос к сайту\n",
    "    2) перебирает блоки со статьями на первой странице поиска\n",
    "    3) для каждой статьи собирает дату, название, ссылку, атора\n",
    "    4) переходит по сстылке статьи и забирает оттуда оценки и полный текст\n",
    "    5) добавляет конкретное поисковое слово\n",
    "    6) доавляет полученную строку к датафрейму\n",
    "    7) пробегает по всем страницам. если передать больше чем надо, ничего не вернется\n",
    "    8) пробегает циклом по всем словам из переданного списка\n",
    "    9) удаляет дубли по столбцу со ссылками\n",
    "    '''\n",
    "    url = 'https://habr.com/ru/search/'\n",
    "    data = pd.DataFrame()\n",
    "    for word in req:\n",
    "        params = {'q': word}\n",
    "        if pages <= get_num_pagins(url, word): # используем вспомогательную функцию для получения максималных страниц\n",
    "            for page in range(1, pages+1):\n",
    "                html = get_html(url+f'page{page}/', params=params) # используем вспомогательную функцию для получения html\n",
    "                soup = BeautifulSoup(html, 'html.parser')\n",
    "                articles = soup.find('div', class_='tm-articles-list').find_all('div', 'tm-article-snippet')\n",
    "                for item in tqdm(articles):\n",
    "                    sleep(0.1)\n",
    "                    date = pd.to_datetime(item.find('time').get('datetime')).strftime('%Y-%m-%d %H:%M') \n",
    "                    title = item.find('a', 'tm-article-snippet__title-link').text\n",
    "                    link = 'https://habr.com' + item.find('a', 'tm-article-snippet__title-link').get('href')\n",
    "                    try:\n",
    "                        votes = get_article_details(link)[0] # используем вспомогательную функцию для получения голосов\n",
    "                    except:\n",
    "                        votes = None\n",
    "                    try:\n",
    "                        text = get_article_details(link)[1] # используем вспомогательную функцию для получения всего текста\n",
    "                    except:\n",
    "                        text = None\n",
    "                    author = item.find('span', 'tm-user-info__user').text.strip()\n",
    "                    row = {'word':word, \n",
    "                           'date':date, \n",
    "                           'title':title, \n",
    "                           'link':link, \n",
    "                           'author':author, \n",
    "                           'votes':votes,\n",
    "                           'text':text}\n",
    "                    data = data.append(row, ignore_index=True)\n",
    "    data.drop_duplicates(subset='link', inplace=True)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "31640d27",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 19/19 [00:17<00:00,  1.10it/s]\n",
      "100%|██████████| 20/20 [00:18<00:00,  1.07it/s]\n",
      "100%|██████████| 19/19 [00:19<00:00,  1.00s/it]\n",
      "100%|██████████| 20/20 [00:16<00:00,  1.21it/s]\n",
      "100%|██████████| 20/20 [00:16<00:00,  1.19it/s]\n",
      "100%|██████████| 20/20 [00:18<00:00,  1.07it/s]\n",
      "100%|██████████| 20/20 [00:20<00:00,  1.02s/it]\n",
      "100%|██████████| 20/20 [00:20<00:00,  1.02s/it]\n",
      "100%|██████████| 20/20 [00:19<00:00,  1.03it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 28.3 s, sys: 853 ms, total: 29.2 s\n",
      "Wall time: 2min 55s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "content_pro = get_content_pro(req, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8273e68f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word</th>\n",
       "      <th>date</th>\n",
       "      <th>title</th>\n",
       "      <th>link</th>\n",
       "      <th>author</th>\n",
       "      <th>votes</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>80</th>\n",
       "      <td>python</td>\n",
       "      <td>2022-05-24 07:57</td>\n",
       "      <td>Дайджест Слёрма: тест на уровень кунг-фу по Py...</td>\n",
       "      <td>https://habr.com/ru/company/southbridge/news/t...</td>\n",
       "      <td>Lika_Chernigo</td>\n",
       "      <td>9.0</td>\n",
       "      <td>Сделали для вас подборку свежих статей и выгод...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74</th>\n",
       "      <td>python</td>\n",
       "      <td>2022-05-18 15:33</td>\n",
       "      <td>Слёрм запускает 3-дневный интенсив по Python д...</td>\n",
       "      <td>https://habr.com/ru/company/southbridge/news/t...</td>\n",
       "      <td>edeshina</td>\n",
       "      <td>5.0</td>\n",
       "      <td>24-26 июня пройдёт онлайн-интенсив для инженер...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76</th>\n",
       "      <td>python</td>\n",
       "      <td>2022-04-22 11:42</td>\n",
       "      <td>TechnoMeetsPython. Онлайн митап о Python-разра...</td>\n",
       "      <td>https://habr.com/ru/news/t/662437/</td>\n",
       "      <td>technokratiya</td>\n",
       "      <td>2.0</td>\n",
       "      <td>27 апреля в 18:00 собираем питонистов на YouTu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85</th>\n",
       "      <td>python</td>\n",
       "      <td>2022-04-18 03:29</td>\n",
       "      <td>Онлайн-митап от руководителя практики Python U...</td>\n",
       "      <td>https://habr.com/ru/company/usetech/news/t/661...</td>\n",
       "      <td>Usetech</td>\n",
       "      <td>0.0</td>\n",
       "      <td>В конце марта Мстислав Казаков, руководитель п...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>большие данные</td>\n",
       "      <td>2022-04-07 14:23</td>\n",
       "      <td>17 лучших инструментов и технологий для работы...</td>\n",
       "      <td>https://habr.com/ru/company/otus/blog/659657/</td>\n",
       "      <td>kmoseenk</td>\n",
       "      <td>7.0</td>\n",
       "      <td>Мир больших данных становится только еще больш...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>119</th>\n",
       "      <td>sql</td>\n",
       "      <td>2022-04-07 08:56</td>\n",
       "      <td>Яндекс Практикум запускает курс «SQL для работ...</td>\n",
       "      <td>https://habr.com/ru/company/yandex_praktikum/n...</td>\n",
       "      <td>eshulyndina</td>\n",
       "      <td>2.0</td>\n",
       "      <td>Яндекс Практикум разработал курс «SQL для рабо...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75</th>\n",
       "      <td>python</td>\n",
       "      <td>2022-03-18 15:31</td>\n",
       "      <td>24 марта Слёрм проведёт открытый урок «Первый ...</td>\n",
       "      <td>https://habr.com/ru/company/southbridge/news/t...</td>\n",
       "      <td>edeshina</td>\n",
       "      <td>14.0</td>\n",
       "      <td>Думаете, написать свою первую программу на Pyt...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64</th>\n",
       "      <td>python</td>\n",
       "      <td>2022-03-08 09:13</td>\n",
       "      <td>Вышел мартовский релиз расширения Python для V...</td>\n",
       "      <td>https://habr.com/ru/news/t/654707/</td>\n",
       "      <td>maybe_elf</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Вышел выпуск расширения Python для Visual Stud...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79</th>\n",
       "      <td>python</td>\n",
       "      <td>2022-02-04 11:04</td>\n",
       "      <td>В Visual Studio Code добавили новые возможност...</td>\n",
       "      <td>https://habr.com/ru/news/t/649717/</td>\n",
       "      <td>daniilshat</td>\n",
       "      <td>9.0</td>\n",
       "      <td>В блоге Microsoft сообщили о нововведениях пла...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>141</th>\n",
       "      <td>sql</td>\n",
       "      <td>2022-01-21 07:00</td>\n",
       "      <td>Карманный справочник: сравнение синтаксиса MS ...</td>\n",
       "      <td>https://habr.com/ru/company/ozontech/blog/645173/</td>\n",
       "      <td>jobgemws</td>\n",
       "      <td>78.0</td>\n",
       "      <td>Приветствую, уважаемые хабражители!Я занимаюсь...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               word              date  \\\n",
       "80           python  2022-05-24 07:57   \n",
       "74           python  2022-05-18 15:33   \n",
       "76           python  2022-04-22 11:42   \n",
       "85           python  2022-04-18 03:29   \n",
       "13   большие данные  2022-04-07 14:23   \n",
       "119             sql  2022-04-07 08:56   \n",
       "75           python  2022-03-18 15:31   \n",
       "64           python  2022-03-08 09:13   \n",
       "79           python  2022-02-04 11:04   \n",
       "141             sql  2022-01-21 07:00   \n",
       "\n",
       "                                                 title  \\\n",
       "80   Дайджест Слёрма: тест на уровень кунг-фу по Py...   \n",
       "74   Слёрм запускает 3-дневный интенсив по Python д...   \n",
       "76   TechnoMeetsPython. Онлайн митап о Python-разра...   \n",
       "85   Онлайн-митап от руководителя практики Python U...   \n",
       "13   17 лучших инструментов и технологий для работы...   \n",
       "119  Яндекс Практикум запускает курс «SQL для работ...   \n",
       "75   24 марта Слёрм проведёт открытый урок «Первый ...   \n",
       "64   Вышел мартовский релиз расширения Python для V...   \n",
       "79   В Visual Studio Code добавили новые возможност...   \n",
       "141  Карманный справочник: сравнение синтаксиса MS ...   \n",
       "\n",
       "                                                  link         author  votes  \\\n",
       "80   https://habr.com/ru/company/southbridge/news/t...  Lika_Chernigo    9.0   \n",
       "74   https://habr.com/ru/company/southbridge/news/t...       edeshina    5.0   \n",
       "76                  https://habr.com/ru/news/t/662437/  technokratiya    2.0   \n",
       "85   https://habr.com/ru/company/usetech/news/t/661...        Usetech    0.0   \n",
       "13       https://habr.com/ru/company/otus/blog/659657/       kmoseenk    7.0   \n",
       "119  https://habr.com/ru/company/yandex_praktikum/n...    eshulyndina    2.0   \n",
       "75   https://habr.com/ru/company/southbridge/news/t...       edeshina   14.0   \n",
       "64                  https://habr.com/ru/news/t/654707/      maybe_elf    0.0   \n",
       "79                  https://habr.com/ru/news/t/649717/     daniilshat    9.0   \n",
       "141  https://habr.com/ru/company/ozontech/blog/645173/       jobgemws   78.0   \n",
       "\n",
       "                                                  text  \n",
       "80   Сделали для вас подборку свежих статей и выгод...  \n",
       "74   24-26 июня пройдёт онлайн-интенсив для инженер...  \n",
       "76   27 апреля в 18:00 собираем питонистов на YouTu...  \n",
       "85   В конце марта Мстислав Казаков, руководитель п...  \n",
       "13   Мир больших данных становится только еще больш...  \n",
       "119  Яндекс Практикум разработал курс «SQL для рабо...  \n",
       "75   Думаете, написать свою первую программу на Pyt...  \n",
       "64   Вышел выпуск расширения Python для Visual Stud...  \n",
       "79   В блоге Microsoft сообщили о нововведениях пла...  \n",
       "141  Приветствую, уважаемые хабражители!Я занимаюсь...  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "content_pro.sort_values(by='date', ascending=False).head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56907ec7",
   "metadata": {},
   "source": [
    "Для одного слова на страницу требуется примерно 16 секунд. По логике скрипт стоит запускать по расписанию ночью. Все 50 страниц поиска должны выгрузиться за примерно 14-15 мин, следовательно 4 слова за 1 час, следовательно за 4 часа можно обработать 16 слов. Если понадобится больше слов, придется пожертвовать страницами. Результат можно сохранить в файл и отправить на почту"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
